{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cathedral-birth",
   "metadata": {},
   "source": [
    "# Classification des chiens (base de standford)\n",
    "\n",
    "Ces quek\n",
    "lques lignes pour donner une implémentation d'un réseau Lent-5 sur des images de chien.\n",
    "\n",
    "Les images sont simplements redimensionnées en 128x128\n",
    "\n",
    "ps : il n'y a pas dans ce code de test (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "through-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-wagon",
   "metadata": {},
   "source": [
    "Chaque image parcourue dans chaque répertoire est redimensionnée pour faire 2 set de données :``\n",
    "\n",
    "Dogs de taille (nb_images,128,128,3)\n",
    "Labels (nb_images, string)  - nom du répertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "musical-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malamute : 178 fichiers\n",
      "Lhasa : 186 fichiers\n",
      "redbone : 148 fichiers\n",
      "Sealyham_terrier : 202 fichiers\n",
      "dingo : 156 fichiers\n",
      "papillon : 196 fichiers\n",
      "curly-coated_retriever : 151 fichiers\n",
      "basset : 175 fichiers\n",
      "Maltese_dog : 252 fichiers\n",
      "standard_schnauzer : 155 fichiers\n",
      "Italian_greyhound : 183 fichiers\n",
      "Welsh_springer_spaniel : 150 fichiers\n",
      "Chihuahua : 152 fichiers\n",
      "Doberman : 150 fichiers\n",
      "Saint_Bernard : 170 fichiers\n",
      "Irish_water_spaniel : 150 fichiers\n",
      "silky_terrier : 183 fichiers\n",
      "Gordon_setter : 153 fichiers\n",
      "groenendael : 150 fichiers\n",
      "English_springer : 159 fichiers\n",
      "Blenheim_spaniel : 188 fichiers\n",
      "Norwich_terrier : 185 fichiers\n",
      "Sussex_spaniel : 151 fichiers\n",
      "West_Highland_white_terrier : 169 fichiers\n",
      "English_setter : 161 fichiers\n",
      "wire-haired_fox_terrier : 157 fichiers\n",
      "miniature_schnauzer : 154 fichiers\n",
      "Samoyed : 218 fichiers\n",
      "Shih-Tzu : 214 fichiers\n",
      "EntleBucher : 202 fichiers\n",
      "vizsla : 154 fichiers\n",
      "Boston_bull : 182 fichiers\n",
      "miniature_pinscher : 184 fichiers\n",
      "Border_collie : 150 fichiers\n",
      "Scottish_deerhound : 232 fichiers\n",
      "Irish_terrier : 169 fichiers\n",
      "kuvasz : 150 fichiers\n",
      "giant_schnauzer : 157 fichiers\n",
      "Yorkshire_terrier : 164 fichiers\n",
      "American_Staffordshire_terrier : 164 fichiers\n",
      "Bernese_mountain_dog : 218 fichiers\n",
      "Tibetan_mastiff : 152 fichiers\n",
      "komondor : 154 fichiers\n",
      "pug : 200 fichiers\n",
      "beagle : 195 fichiers\n",
      "African_hunting_dog : 169 fichiers\n",
      "cocker_spaniel : 159 fichiers\n",
      "Ibizan_hound : 188 fichiers\n",
      "Bouvier_des_Flandres : 150 fichiers\n",
      "bloodhound : 187 fichiers\n",
      "French_bulldog : 159 fichiers\n",
      "miniature_poodle : 155 fichiers\n",
      "basenji : 209 fichiers\n",
      "Bedlington_terrier : 182 fichiers\n",
      "German_shepherd : 152 fichiers\n",
      "Cardigan : 155 fichiers\n",
      "toy_poodle : 151 fichiers\n",
      "Chesapeake_Bay_retriever : 167 fichiers\n",
      "Saluki : 200 fichiers\n",
      "Siberian_husky : 192 fichiers\n",
      "Airedale : 202 fichiers\n",
      "Old_English_sheepdog : 169 fichiers\n",
      "Appenzeller : 151 fichiers\n",
      "bull_mastiff : 156 fichiers\n",
      "Shetland_sheepdog : 157 fichiers\n",
      "bluetick : 171 fichiers\n",
      "Afghan_hound : 239 fichiers\n",
      "Leonberg : 210 fichiers\n",
      "Great_Pyrenees : 213 fichiers\n",
      "Staffordshire_bullterrier : 155 fichiers\n",
      "standard_poodle : 159 fichiers\n",
      "Labrador_retriever : 171 fichiers\n",
      "Eskimo_dog : 150 fichiers\n",
      "malinois : 150 fichiers\n",
      "dhole : 150 fichiers\n",
      "boxer : 151 fichiers\n",
      "Irish_wolfhound : 218 fichiers\n",
      "Irish_setter : 155 fichiers\n",
      "Walker_hound : 153 fichiers\n",
      "schipperke : 154 fichiers\n",
      "Dandie_Dinmont : 180 fichiers\n",
      "otterhound : 151 fichiers\n",
      "golden_retriever : 150 fichiers\n",
      "collie : 153 fichiers\n",
      "Kerry_blue_terrier : 179 fichiers\n",
      "black-and-tan_coonhound : 159 fichiers\n",
      "Pomeranian : 219 fichiers\n",
      "Lakeland_terrier : 197 fichiers\n",
      "affenpinscher : 150 fichiers\n",
      "Tibetan_terrier : 206 fichiers\n",
      "Rottweiler : 152 fichiers\n",
      "cairn : 197 fichiers\n",
      "English_foxhound : 157 fichiers\n",
      "chow : 196 fichiers\n",
      "Greater_Swiss_Mountain_dog : 168 fichiers\n",
      "Brabancon_griffon : 153 fichiers\n",
      "briard : 152 fichiers\n",
      "Australian_terrier : 196 fichiers\n",
      "clumber : 150 fichiers\n",
      "Weimaraner : 160 fichiers\n",
      "Pembroke : 181 fichiers\n",
      "Mexican_hairless : 155 fichiers\n",
      "kelpie : 153 fichiers\n",
      "whippet : 187 fichiers\n",
      "soft-coated_wheaten_terrier : 156 fichiers\n",
      "Japanese_spaniel : 185 fichiers\n",
      "Norfolk_terrier : 172 fichiers\n",
      "keeshond : 158 fichiers\n",
      "Scotch_terrier : 158 fichiers\n",
      "Great_Dane : 156 fichiers\n",
      "Border_terrier : 172 fichiers\n",
      "borzoi : 151 fichiers\n",
      "Newfoundland : 195 fichiers\n",
      "German_short-haired_pointer : 152 fichiers\n",
      "flat-coated_retriever : 152 fichiers\n",
      "Rhodesian_ridgeback : 172 fichiers\n",
      "Brittany_spaniel : 152 fichiers\n",
      "toy_terrier : 172 fichiers\n",
      "Pekinese : 149 fichiers\n",
      "Norwegian_elkhound : 196 fichiers\n"
     ]
    }
   ],
   "source": [
    "im_size=128\n",
    "nb_classes=120\n",
    "Dogs=[];\n",
    "Labels=[]\n",
    "Categories = os.listdir('Images')\n",
    "for cat in Categories[:nb_classes] :\n",
    "    Files = os.listdir('Images/'+cat)\n",
    "    dog_name=cat[cat.find('-')+1:]\n",
    "    print(dog_name+ ' : '+str(len(Files))+' fichiers')\n",
    "    for f in Files :\n",
    "        if f=='.ipynb_checkpoints':\n",
    "            continue\n",
    "        im = cv2.imread('Images/'+cat+'/'+f)\n",
    "        im=  cv2.resize(im, (im_size, im_size),\n",
    "                        interpolation=cv2.INTER_NEAREST)\n",
    "        Dogs.append(im)\n",
    "        Labels.append(dog_name)\n",
    "Dogs=np.array(Dogs)\n",
    "Labels=np.array(Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-amendment",
   "metadata": {},
   "source": [
    "### One  hot encoder\n",
    "\n",
    "à l'aide de LabelEncoder de Scikit et fit_transform de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adolescent-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Vec = label_encoder.fit_transform(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "sitting-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = keras.utils.to_categorical(Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-snowboard",
   "metadata": {},
   "source": [
    "### Modèle Lenet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "varied-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 128, 128, 6)       456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 64, 64, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 64, 64, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 120)       48120     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 28, 28, 120)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 94080)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 84)                7902804   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 120)               10200     \n",
      "=================================================================\n",
      "Total params: 7,963,996\n",
      "Trainable params: 7,963,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Conv2D(number_filters, kernel_size, input_shape=(number_channels, img_col), padding, activation)\n",
    "model.add(keras.layers.Conv2D(6, (5, 5 ), input_shape=[im_size, im_size , 3], padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(16, (5, 5 ), padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(120, (5, 5 ), activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(84, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(nb_classes, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-radar",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "becoming-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "206/206 [==============================] - 108s 513ms/step - loss: 110330654677.1671 - accuracy: 0.0108\n",
      "Epoch 2/30\n",
      "206/206 [==============================] - 87s 424ms/step - loss: 4.7872 - accuracy: 0.0124\n",
      "Epoch 3/30\n",
      "206/206 [==============================] - 100s 487ms/step - loss: 4.7869 - accuracy: 0.0123\n",
      "Epoch 4/30\n",
      "206/206 [==============================] - 104s 507ms/step - loss: 4.7865 - accuracy: 0.0119\n",
      "Epoch 5/30\n",
      "206/206 [==============================] - 109s 529ms/step - loss: 4.7863 - accuracy: 0.0128\n",
      "Epoch 6/30\n",
      "206/206 [==============================] - 101s 490ms/step - loss: 4.7860 - accuracy: 0.0122\n",
      "Epoch 7/30\n",
      "206/206 [==============================] - 99s 479ms/step - loss: 4.7857 - accuracy: 0.0121\n",
      "Epoch 8/30\n",
      "206/206 [==============================] - 104s 504ms/step - loss: 4.7854 - accuracy: 0.0126\n",
      "Epoch 9/30\n",
      "206/206 [==============================] - 107s 518ms/step - loss: 4.7852 - accuracy: 0.0115\n",
      "Epoch 10/30\n",
      "206/206 [==============================] - 104s 504ms/step - loss: 4.7850 - accuracy: 0.0124\n",
      "Epoch 11/30\n",
      "206/206 [==============================] - 101s 490ms/step - loss: 4.7848 - accuracy: 0.0133\n",
      "Epoch 12/30\n",
      "206/206 [==============================] - 110s 536ms/step - loss: 4.7849 - accuracy: 0.0114\n",
      "Epoch 13/30\n",
      "206/206 [==============================] - 101s 491ms/step - loss: 4.7843 - accuracy: 0.0119\n",
      "Epoch 14/30\n",
      "206/206 [==============================] - 103s 500ms/step - loss: 4.7843 - accuracy: 0.0114\n",
      "Epoch 15/30\n",
      "206/206 [==============================] - 103s 502ms/step - loss: 4.7841 - accuracy: 0.0126\n",
      "Epoch 16/30\n",
      "206/206 [==============================] - 104s 506ms/step - loss: 4.7836 - accuracy: 0.0128\n",
      "Epoch 17/30\n",
      "206/206 [==============================] - 123s 597ms/step - loss: 4.7836 - accuracy: 0.0124\n",
      "Epoch 18/30\n",
      "206/206 [==============================] - 102s 495ms/step - loss: 4.7837 - accuracy: 0.0122\n",
      "Epoch 19/30\n",
      "206/206 [==============================] - 103s 500ms/step - loss: 4.7831 - accuracy: 0.0119\n",
      "Epoch 20/30\n",
      "206/206 [==============================] - 106s 515ms/step - loss: 4.7832 - accuracy: 0.0108\n",
      "Epoch 21/30\n",
      "206/206 [==============================] - 104s 503ms/step - loss: 4.7830 - accuracy: 0.0117\n",
      "Epoch 22/30\n",
      "206/206 [==============================] - 106s 515ms/step - loss: 4.7831 - accuracy: 0.0126\n",
      "Epoch 23/30\n",
      "206/206 [==============================] - 104s 506ms/step - loss: 4.7826 - accuracy: 0.0116\n",
      "Epoch 24/30\n",
      "206/206 [==============================] - 102s 496ms/step - loss: 4.7827 - accuracy: 0.0129\n",
      "Epoch 25/30\n",
      "206/206 [==============================] - 103s 500ms/step - loss: 4.7826 - accuracy: 0.0126\n",
      "Epoch 26/30\n",
      "206/206 [==============================] - 101s 493ms/step - loss: 4.7823 - accuracy: 0.0130\n",
      "Epoch 27/30\n",
      "206/206 [==============================] - 102s 494ms/step - loss: 4.7824 - accuracy: 0.0123\n",
      "Epoch 28/30\n",
      " 66/206 [========>.....................] - ETA: 1:08 - loss: 4.7810 - accuracy: 0.0123"
     ]
    }
   ],
   "source": [
    "nb_epoch = 30 # nombre de passes pour chaque images d'entrainement\n",
    "batch_size = 100 # taille du lot (Gradient)\n",
    "\n",
    "model.fit(Dogs, labels, batch_size=batch_size, epochs=nb_epoch, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "score =  model.evaluate(Dogs, labels, verbose=0)\n",
    "print('Score', score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-bangkok",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
